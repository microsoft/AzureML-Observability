{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (pyarrow 7.0.0 (c:\\users\\janguy\\anaconda3\\envs\\dlresearch\\lib\\site-packages), Requirement.parse('pyarrow<4.0.0,>=0.17.0'), {'azureml-dataset-runtime'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (azureml-core 1.39.0 (c:\\users\\janguy\\anaconda3\\envs\\dlresearch\\lib\\site-packages), Requirement.parse('azureml-core~=1.38.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (azureml-core 1.39.0 (c:\\users\\janguy\\anaconda3\\envs\\dlresearch\\lib\\site-packages), Requirement.parse('azureml-core~=1.38.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (azureml-core 1.39.0 (c:\\users\\janguy\\anaconda3\\envs\\dlresearch\\lib\\site-packages), Requirement.parse('azureml-core~=1.38.0')).\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "from obs.management import provision,set_adx_to_workspace\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from obs.collector import Online_Collector\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provision Resources\n",
    "\n",
    "#### Create ADX Cluster, Service Principal, and store value's in Azure Machine Learning's Key Vault for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provision with default SKU of Standard_D11_v2, Standard Tier of ADX cluster. The provion will also create a service principal as part of the process.\n",
    "#You need to have right to provision service principal in the subscription.\n",
    "provision(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup a service principal, ADX cluster, database in seperate step and attach the ADX cluster to Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setup ADX cluster:\n",
    "- Create your own ADX cluster. The cluster has to be enabled with Streaming Ingestion (https://docs.microsoft.com/en-us/azure/data-explorer/ingest-data-streaming?tabs=azure-portal%2Ccsharp) and Python language extension https://docs.microsoft.com/en-us/azure/data-explorer/language-extensions\n",
    "- Create or reuse an existing service principal\n",
    "- Create a database\n",
    "- Assign the service principal to the database as admin https://docs.microsoft.com/en-us/azure/data-explorer/manage-database-permissions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Attach the cluster to Azure ML workspace\n",
    "Prepare cluster_uri (e.g. https://adx02.westus2.kusto.windows.net),db_name, client_id, client_secret, subscription_id, tenant_id and run the following command to attach the cluster to Azure ML workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_adx_to_workspace(ws, cluster_uri,db_name, client_id, client_secret, subscription_id, tenant_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "Once the resources are created, data can now be ingested to Azure Data Explorer. To use the dashboards, the data must have a timestamp column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw data\n",
    "\n",
    "dataset = pd.read_csv(\"https://azuremlexamples.blob.core.windows.net/datasets/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".create table IRIS_DATA_NEW (['sepal_length']: real, ['sepal_width']: real, ['petal_length']: real, ['petal_width']: real, ['species']: string, ['timestamp']: datetime)\n"
     ]
    }
   ],
   "source": [
    "# Add timestamp column\n",
    "from obs.collector import Online_Collector\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"https://azuremlexamples.blob.core.windows.net/datasets/iris.csv\")\n",
    "\n",
    "dataset[\"timestamp\"] =  [pd.to_datetime('now') - timedelta(days=x) for x in range(len(dataset))]\n",
    "table_name = \"IRIS_DATA_NEW\" #new dataset\n",
    "\n",
    "online_collector = Online_Collector(table_name,ws=ws)\n",
    "online_collector.batch_collect(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Ingestion\n",
    "\n",
    "Entire dataframe will be loaded into ADX at once as a table named ```irisdata```. The is also a stream ingestion available to ingest data asynchronously with an internal buffering mechanism. This method can be utilized to lower impact to main scoring thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".create table irisdata (['sepal_length']: real, ['sepal_width']: real, ['petal_length']: real, ['petal_width']: real, ['species']: string, ['datetime']: datetime)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "table_name = \"irisdata\" #new dataset\n",
    "\n",
    "online_collector = Online_Collector(table_name,ws=ws)\n",
    "online_collector.batch_collect(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Ingestion (run this in Databricks or Synapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Databricks or Synapse Spark, install the library:\n",
    "\n",
    "```pip install --upgrade git+https://github.com/microsoft/AzureML-Observability#subdirectory=aml-obs-collector```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logon with a service principal so that you can run this as a job. You can also logon w9th interactive  mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "from azureml.core import Workspace\n",
    "\n",
    "sp_auth = ServicePrincipalAuthentication(tenant_id =tenant_id,\n",
    "                                       service_principal_id=service_principal_id,\n",
    "                                       service_principal_password=service_principal_password)\n",
    "# Instantiate Azure Machine Learning workspace\n",
    "ws = Workspace.get(name=workspace_name,\n",
    "                   subscription_id=subscription_id,\n",
    "                   resource_group=resource_group,auth= sp_auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =spark.read.format(\"csv\").option(\"header\", True).load(\"wasbs://ojsales-simulatedcontainer@azureopendatastorage.blob.core.windows.net/oj_sales_data/Store10*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obs.collector import spark_collect\n",
    "\n",
    "table_name = \"adb_oj_sales\"\n",
    "spark_collect(data,table_name,ws)\n",
    "#will take a few minutes for result to show up in ADX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real time ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the example in monitoring notebook to see how real time ingestion works"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8858a4df92b06e9052bc306608e3218c33233584bc6448961c72d65ba55843de"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('dlresearch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
