{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from obs.management import provision,set_adx_to_workspace\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from obs.collector import Online_Collector\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provision Resources\n",
    "\n",
    "##### Option 1: Let the provisioning process setup everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Provision with default SKU of Standard_D11_v2, Standard Tier of ADX cluster.\n",
    "- You need to have right to provision service principal in the subscription. Run below command to test if you can create Service Principal. Replace sp_name with any name, and fill in subscription_id and resource_group_name. <br>\n",
    " az ad sp create-for-rbac --name {sp_name} --role contributor --scopes /subscriptions/{subscription_id}/resourceGroups/{resource_group_name} --sdk-auth\n",
    "- Make sure resource provider Microsoft.Kusto is registered in your subscription.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if the login screen does not pop-up, please copy and run the following command to login\n",
      "az login --tenant 72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "Creating Service Principal\n",
      "CompletedProcess(args='az ad sp create-for-rbac --name ws01ent_ws01entmonitor51_monitoringsp --role contributor --scopes /subscriptions/0e9bace8-7a81-4922-83b5-d995ff706507/resourceGroups/azureml --sdk-auth', returncode=0, stdout=b'{\\r\\n  \"clientId\": \"a5891756-bde3-4235-b145-d6379919876c\",\\r\\n  \"clientSecret\": \"ROLWI~qsQD_TNQ5ZA4d3wXULsxqahRCEmH\",\\r\\n  \"subscriptionId\": \"0e9bace8-7a81-4922-83b5-d995ff706507\",\\r\\n  \"tenantId\": \"72f988bf-86f1-41af-91ab-2d7cd011db47\",\\r\\n  \"activeDirectoryEndpointUrl\": \"https://login.microsoftonline.com\",\\r\\n  \"resourceManagerEndpointUrl\": \"https://management.azure.com/\",\\r\\n  \"activeDirectoryGraphResourceId\": \"https://graph.windows.net/\",\\r\\n  \"sqlManagementEndpointUrl\": \"https://management.core.windows.net:8443/\",\\r\\n  \"galleryEndpointUrl\": \"https://gallery.azure.com/\",\\r\\n  \"managementEndpointUrl\": \"https://management.core.windows.net/\"\\r\\n}\\r\\n')\n",
      "begin creating ADX cluster ws01entmonitor51 at westus2 with Standard_D11_v2 and capacity 2\n",
      "finished creating cluster ws01entmonitor51\n",
      "Beging enabling Python for ws01entmonitor51\n",
      "Finished enabling Python for ws01entmonitor51\n",
      "begin creating DB mlmonitoring for cluster ws01entmonitor51\n",
      "finished creating database\n",
      "finished assigning SP to database\n"
     ]
    }
   ],
   "source": [
    "\n",
    "provision(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Option 2: Bring your own service principal and ADX cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setup ADX cluster:\n",
    "- Create your own ADX cluster. The cluster has to be enabled with Streaming Ingestion (https://docs.microsoft.com/en-us/azure/data-explorer/ingest-data-streaming?tabs=azure-portal%2Ccsharp) and Python language extension https://docs.microsoft.com/en-us/azure/data-explorer/language-extensions\n",
    "- Create or reuse an existing service principal\n",
    "- Create a database\n",
    "- Assign the service principal to the database as admin https://docs.microsoft.com/en-us/azure/data-explorer/manage-database-permissions\n",
    "- Assign yourself to the database so that you can query \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Attach the cluster to Azure ML workspace\n",
    "Prepare cluster_uri (e.g. https://adx02.westus2.kusto.windows.net),db_name, client_id, client_secret, subscription_id, tenant_id and run the following command to attach the cluster to Azure ML workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv = ws.get_default_keyvault()\n",
    "tenant_id = \"72f988bf-86f1-41af-91ab-2d7cd011db47\"\n",
    "client_id = \"af883abf-89dd-4889-bdb3-1ee84f68465e\"\n",
    "client_secret = kv.get_secret(client_id)\n",
    "subscription_id = \"0e9bace8-7a81-4922-83b5-d995ff706507\"\n",
    "cluster_uri = \"https://adx02.westus2.kusto.windows.net\" #URL of the ADX Cluster\n",
    "\n",
    "db_name = \"db01\"\n",
    "\n",
    "set_adx_to_workspace(ws, cluster_uri,db_name, client_id, client_secret, subscription_id, tenant_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "Once the resources are created, data can now be ingested to Azure Data Explorer. To use the dashboards, the data must have a timestamp column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw data\n",
    "\n",
    "dataset = pd.read_csv(\"https://azuremlexamples.blob.core.windows.net/datasets/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".create table IRIS_DATA_NEW (['sepal_length']: real, ['sepal_width']: real, ['petal_length']: real, ['petal_width']: real, ['species']: string, ['timestamp']: datetime)\n"
     ]
    }
   ],
   "source": [
    "# Add timestamp column\n",
    "from obs.collector import Online_Collector\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"https://azuremlexamples.blob.core.windows.net/datasets/iris.csv\")\n",
    "\n",
    "dataset[\"timestamp\"] =  [pd.to_datetime('now') - timedelta(days=x) for x in range(len(dataset))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Ingestion\n",
    "\n",
    "Entire dataframe will be loaded into ADX at once as a table named ```irisdata```. The is also a stream ingestion available to ingest data asynchronously with an internal buffering mechanism. This method can be utilized to lower impact to main scoring thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "table_name = \"IRIS_DATA_NEW\" #new dataset\n",
    "\n",
    "online_collector = Online_Collector(table_name,ws=ws)\n",
    "online_collector.batch_collect(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Ingestion (run this in Databricks or Synapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Databricks or Synapse Spark, install the library:\n",
    "\n",
    "```pip install --upgrade git+https://github.com/microsoft/AzureML-Observability#subdirectory=aml-obs-collector```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logon with a service principal so that you can run this as a job. You can also logon w9th interactive  mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "from azureml.core import Workspace\n",
    "\n",
    "sp_auth = ServicePrincipalAuthentication(tenant_id =tenant_id,\n",
    "                                       service_principal_id=service_principal_id,\n",
    "                                       service_principal_password=service_principal_password)\n",
    "# Instantiate Azure Machine Learning workspace\n",
    "ws = Workspace.get(name=workspace_name,\n",
    "                   subscription_id=subscription_id,\n",
    "                   resource_group=resource_group,auth= sp_auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =spark.read.format(\"csv\").option(\"header\", True).load(\"wasbs://ojsales-simulatedcontainer@azureopendatastorage.blob.core.windows.net/oj_sales_data/Store10*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obs.collector import spark_collect\n",
    "\n",
    "table_name = \"adb_oj_sales\"\n",
    "spark_collect(data,table_name,ws)\n",
    "#will take a few minutes for result to show up in ADX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real time ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the example in monitoring notebook to see how real time ingestion works"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8858a4df92b06e9052bc306608e3218c33233584bc6448961c72d65ba55843de"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('dlresearch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
